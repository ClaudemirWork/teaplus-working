// Arquivo: utils/gameAudioManager.ts

// A palavra "export" aqui garante que estamos exportando a CLASSE,
// o que nos dÃ¡ acesso ao mÃ©todo estÃ¡tico .getInstance()
export class GameAudioManager {
Â  private static instance: GameAudioManager;
Â  private isEnabled: boolean = true;
Â  
  // As chaves agora sÃ£o lidas das variÃ¡veis de ambiente
Â  private azureKey: string;
Â  private azureRegion: string;

Â  private audioContext: AudioContext | null = null;
Â  private isInitialized: boolean = false;

Â  constructor() {
    // As variÃ¡veis de ambiente sÃ£o carregadas aqui quando a classe Ã© instanciada
Â  Â  this.azureKey = process.env.NEXT_PUBLIC_AZURE_SPEECH_KEY || '';
Â  Â  this.azureRegion = process.env.NEXT_PUBLIC_AZURE_SPEECH_REGION || '';

Â  Â  console.log('ğŸ”Š GameAudioManager inicializado');

    // Checagem para garantir que as chaves foram carregadas
Â  Â  if (!this.azureKey || !this.azureRegion) {
Â  Â  Â  console.error('âŒ ERRO CRÃTICO: Chaves do Azure nÃ£o encontradas nas variÃ¡veis de ambiente!');
Â  Â  Â  console.log('Verifique o arquivo .env.local (para ambiente local) ou as configuraÃ§Ãµes do Netlify (para produÃ§Ã£o).');
Â  Â  } else {
Â  Â  Â  console.log('Azure Key vÃ¡lida:', this.azureKey.substring(0, 4) + '...');
Â  Â  Â  console.log('Azure Region:', this.azureRegion);
Â  Â  }
Â  }

  // Este mÃ©todo estÃ¡tico precisa da CLASSE para ser chamado, por isso o "export class" Ã© crucial
Â  static getInstance(): GameAudioManager {
Â  Â  if (!GameAudioManager.instance) {
Â  Â  Â  GameAudioManager.instance = new GameAudioManager();
Â  Â  }
Â  Â  return GameAudioManager.instance;
Â  }

Â  private async initializeAudioContext(): Promise<void> {
Â  Â  if (this.isInitialized && this.audioContext?.state === 'running') {
Â  Â  Â  return;
Â  Â  }

Â  Â  try {
Â  Â  Â  if (!this.audioContext) {
Â  Â  Â  Â  this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
Â  Â  Â  Â  console.log('ğŸµ AudioContext criado:', this.audioContext.state);
Â  Â  Â  }

Â  Â  Â  if (this.audioContext.state === 'suspended') {
Â  Â  Â  Â  console.log('ğŸ”“ Resumindo AudioContext...');
Â  Â  Â  Â  await this.audioContext.resume();
Â  Â  Â  Â  console.log('âœ… AudioContext resumed:', this.audioContext.state);
Â  Â  Â  }

Â  Â  Â  this.isInitialized = true;
Â  Â  Â  console.log('âœ… AudioContext inicializado e pronto');
Â  Â  } catch (error) {
Â  Â  Â  console.error('âŒ Erro ao inicializar AudioContext:', error);
Â  Â  Â  throw error;
Â  Â  }
Â  }

Â  private async synthesizeAzureSpeech(texto: string, voiceName: string): Promise<ArrayBuffer> {
Â  Â  const endpoint = `https://${this.azureRegion}.tts.speech.microsoft.com/cognitiveservices/v1`;
Â  Â Â 
Â  Â  console.log('ğŸ¤ Azure TTS Request:', {
Â  Â  Â  endpoint,
Â  Â  Â  voiceName,
Â  Â  Â  texto: texto.substring(0, 50) + '...',
Â  Â  Â  region: this.azureRegion
Â  Â  });

Â  Â  const ssml = `<speak version='1.0' xml:lang='pt-BR' xmlns='http://www.w3.org/2001/10/synthesis'>
Â  <voice name='${voiceName}'>
Â  Â  <prosody rate="0.9" pitch="medium">
Â  Â  Â  ${texto.replace(/[<>&"']/g, (match) => {
Â  Â  Â  Â  const entities: {[key: string]: string} = {
Â  Â  Â  Â  Â  '<': '&lt;',
Â  Â  Â  Â  Â  '>': '&gt;',
Â  Â  Â  Â  Â  '&': '&amp;',
Â  Â  Â  Â  Â  '"': '&quot;',
Â  Â  Â  Â  Â  "'": '&apos;'
Â  Â  Â  Â  };
Â  Â  Â  Â  return entities[match];
Â  Â  Â  })}
Â  Â  </prosody>
Â  </voice>
</speak>`;

Â  Â  try {
Â  Â  Â  const response = await fetch(endpoint, {
Â  Â  Â  Â  method: 'POST',
Â  Â  Â  Â  headers: {
Â  Â  Â  Â  Â  'Ocp-Apim-Subscription-Key': this.azureKey,
Â  Â  Â  Â  Â  'Content-Type': 'application/ssml+xml',
Â  Â  Â  Â  Â  'X-Microsoft-OutputFormat': 'audio-16khz-128kbitrate-mono-mp3',
Â  Â  Â  Â  Â  'User-Agent': 'LudiTEA-BubblePop/1.0',
Â  Â  Â  Â  Â  'Accept': '*/*',
Â  Â  Â  Â  Â  'Cache-Control': 'no-cache'
Â  Â  Â  Â  },
Â  Â  Â  Â  body: ssml
Â  Â  Â  });

Â  Â  Â  console.log('ğŸ“¡ Azure Response:', {
Â  Â  Â  Â  status: response.status,
Â  Â  Â  Â  statusText: response.statusText,
Â  Â  Â  Â  ok: response.ok,
Â  Â  Â  });

Â  Â  Â  if (!response.ok) {
Â  Â  Â  Â  const errorText = await response.text();
Â  Â  Â  Â  console.error('âŒ Azure TTS Error Details:', {
Â  Â  Â  Â  Â  status: response.status,
Â  Â  Â  Â  Â  statusText: response.statusText,
Â  Â  Â  Â  Â  error: errorText,
Â  Â  Â  Â  Â  endpoint
Â  Â  Â  Â  });
Â  Â  Â  Â Â 
Â  Â  Â  Â  if (response.status === 401) {
Â  Â  Â  Â  Â  throw new Error('Chave do Azure invÃ¡lida ou expirada');
Â  Â  Â  Â  }
Â  Â  Â  Â  throw new Error(`Azure TTS Error: ${response.status} - ${errorText}`);
Â  Â  Â  }

Â  Â  Â  const arrayBuffer = await response.arrayBuffer();
Â  Â  Â  if (arrayBuffer.byteLength === 0) {
Â  Â  Â  Â  throw new Error('Azure retornou Ã¡udio vazio');
Â  Â  Â  }
Â  Â  Â  console.log('âœ… Azure TTS Success - Audio size:', arrayBuffer.byteLength, 'bytes');
Â  Â  Â  return arrayBuffer;

Â  Â  } catch (error: any) {
Â  Â  Â  console.error('âŒ Azure TTS Network Error:', error);
Â  Â  Â  throw new Error(`Erro de conexÃ£o Azure: ${error.message}`);
Â  Â  }
Â  }

Â  private async playAudioBuffer(buffer: ArrayBuffer, callback?: () => void): Promise<void> {
Â  Â  try {
Â  Â  Â  await this.initializeAudioContext();
Â  Â  Â  if (!this.audioContext) throw new Error('AudioContext nÃ£o disponÃ­vel');

Â  Â  Â  const audioBuffer = await this.audioContext.decodeAudioData(buffer.slice(0));
Â  Â  Â  const source = this.audioContext.createBufferSource();
Â  Â  Â  source.buffer = audioBuffer;
Â  Â  Â  source.connect(this.audioContext.destination);
Â  Â  Â Â 
Â  Â  Â  source.onended = () => {
Â  Â  Â  Â  console.log('ğŸµ ReproduÃ§Ã£o finalizada');
Â  Â  Â  Â  callback?.();
Â  Â  Â  };
Â  Â  Â Â 
Â  Â  Â  source.start(0);
Â  Â  Â  console.log('â–¶ï¸ ReproduÃ§Ã£o iniciada');
Â  Â  } catch (error: any) {
Â  Â  Â  console.error('âŒ Erro ao reproduzir Ã¡udio:', error);
Â  Â  Â  callback?.();
Â  Â  Â  throw new Error(`Erro de reproduÃ§Ã£o: ${error.message}`);
Â  Â  }
Â  }

Â  async falarMila(texto: string, callback?: () => void): Promise<void> {
Â  Â  if (!this.isEnabled || !texto || texto.trim().length === 0) {
Â  Â  Â  callback?.();
Â  Â  Â  return;
Â  Â  }

Â  Â  console.log('ğŸ¤ Iniciando fala da Mila:', texto);
Â  Â  try {
Â  Â  Â  const audioBuffer = await this.synthesizeAzureSpeech(texto, 'pt-BR-FranciscaNeural');
Â  Â  Â  await this.playAudioBuffer(audioBuffer, callback);
Â  Â  Â  console.log('âœ… Fala da Mila concluÃ­da com sucesso');
Â  Â  } catch (error: any) {
Â  Â  Â  console.error('âŒ Erro completo na fala da Mila:', error);
Â  Â  Â  callback?.();
Â  Â  }
Â  }

Â  async falarLeo(texto: string, callback?: () => void): Promise<void> {
Â  Â  if (!this.isEnabled || !texto || texto.trim().length === 0) {
Â  Â  Â  callback?.();
Â  Â  Â  return;
Â  Â  }

Â  Â  console.log('ğŸ¤ Iniciando fala do Leo:', texto);
Â  Â  try {
Â  Â  Â  const audioBuffer = await this.synthesizeAzureSpeech(texto, 'pt-BR-AntonioNeural');
Â  Â  Â  await this.playAudioBuffer(audioBuffer, callback);
Â  Â  Â  console.log('âœ… Fala do Leo concluÃ­da com sucesso');
Â  Â  } catch (error: any) {
Â  Â  Â  console.error('âŒ Erro completo na fala do Leo:', error);
Â  Â  Â  callback?.();
Â  Â  }
Â  }

Â  pararTodos(): void {
Â  Â  console.log('ğŸ›‘ Parando todas as reproduÃ§Ãµes');
Â  Â  if (this.audioContext && this.audioContext.state === 'running') {
Â  Â  Â  this.audioContext.close().then(() => {
Â  Â  Â  Â  this.audioContext = null;
Â  Â  Â  Â  this.isInitialized = false;
Â  Â  Â  Â  console.log('AudioContext fechado e resetado.');
Â  Â  Â  });
Â  Â  }
Â  }

Â  toggleAudio(): boolean {
Â  Â  this.isEnabled = !this.isEnabled;
Â  Â  console.log('ğŸ”Š Ãudio', this.isEnabled ? 'LIGADO' : 'DESLIGADO');
Â  Â  if (!this.isEnabled) {
Â  Â  Â  this.pararTodos();
Â  Â  }
Â  Â  return this.isEnabled;
Â  }

Â  isAudioEnabled(): boolean {
Â  Â  return this.isEnabled;
Â  }
Â  
Â  getAudioContextState(): string {
Â  Â  return this.audioContext?.state || 'not-initialized';
Â  }
Â  
Â  async forceInitialize(): Promise<void> {
Â  Â  await this.initializeAudioContext();
Â  }
}
